{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13914084,"sourceType":"datasetVersion","datasetId":8865807}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"üß≠ JobPilot ‚Äî An Autonomous Multi-Agent Job Search & Application System\n\nJobPilot is an end-to-end, fully autonomous multi-agent system built using the Google Agent Development Kit (ADK) and powered by Gemini 2.5 models.\nIts mission is simple:\n\nAutomatically understand the user‚Äôs background, discover relevant jobs, evaluate them, summarize them, and generate complete tailored application packages ‚Äî all inside one unified pipeline.\n\nThis notebook contains the full implementation, from architecture to ingestion to multi-agent coordination.\n\nüéØ Project Overview\n\nSearching for jobs, matching them to your skills, filtering out irrelevant postings, and generating tailored resumes/cover letters is slow and repetitive. JobPilot automates all of this.\n\nGiven only free-form text from the user, JobPilot:\n\n‚úî Builds or updates a structured professional profile\n‚úî Retrieves job postings from a vector database\n‚úî Filters and ranks opportunities using agents\n‚úî Summarizes job descriptions clearly for the user\n‚úî Generates tailored resumes and cover letters\n‚úî Produces complete application packages for selected jobs\n\nThe entire pipeline runs autonomously through coordinated agents.\n\nüß† Architecture Summary\n\nJobPilot follows a modular multi-agent design, where each agent handles a focused skillset:\n\n1. Orchestrator Agent\n\n   The system‚Äôs ‚Äúbrain.‚Äù\n   Routes all information, manages tools, oversees sequencing, and enforces system rules.\n   It never performs work itself ‚Äî it delegates everything to sub-agents.\n\n2. Job Search Agent (Agent 1)\n\n   Retrieves and ranks relevant job postings via ChromaDB.\n   Performs:\n\n   Dense semantic search\n\n   Duplicate/rejection filtering\n\n   Job scoring (via job_filter_agent)\n\n   Final ranking (via rank_job_tool)\n\n3. Application Builder Agent (Agent 2)\n\n   For every selected job, generates:\n\n   A tailored resume\n\n   A tailored cover letter\n   using its dedicated sub-agents.\n\n4. Supporting Agents\n\n   profile_builder_agent ‚Äî parses/updates the user‚Äôs background\n\n   job_filter_agent ‚Äî evaluates user‚Äìjob fit\n\n   job_summarizer_agent ‚Äî transforms job data into clear summaries\n\n   resume_generator_agent ‚Äî produces job-specific resumes\n\n   cover_letter_generator_agent ‚Äî produces job-specific cover letters\n\n   All agents share unified schemas and follow strict input/output constraints.\n\nüì¶ Data & Vector Search Layer\n\nJobPilot uses ChromaDB as its semantic retrieval engine, storing job postings with structured metadata and vector embeddings.\nThis layer powers the entire job-search experience inside the multi-agent system.\n\nThis notebook includes:\n\n‚úî Standalone ingestion pipeline (ingest_jobs.py)\n\n‚úî HTML extraction using BeautifulSoup\n\n‚úî Normalization into the unified JOB_DETAILS_SCHEMA\n\n‚úî Embedding generation (local SentenceTransformers or Gemini embeddings)\n\n‚úî Persistent ChromaDB storage for all job postings\n\n‚úî Semantic retrieval used exclusively by the Job Search Agent\n\nThis forms the foundation for fast, intelligent job discovery.\n\nüõ†Ô∏è Core Technologies Used\n\nJobPilot combines modern agent tooling with vector search and structured data pipelines:\n\n‚úî Google ADK ‚Äî agents, tools, sessions, runners\n\n‚úî Gemini 2.5 Flash / Flash-Lite ‚Äî reasoning, classification, generation\n\n‚úî ChromaDB ‚Äî persistent vector store with metadata\n\n‚úî SentenceTransformers ‚Äî optional local embeddings (ingestion)\n\n‚úî SQLite (DatabaseSessionService) ‚Äî long-term session memory\n\n‚úî BeautifulSoup4 ‚Äî structured HTML parsing\n\n‚úî Python, Pydantic, asyncio ‚Äî core runtime, validation, async execution\n\n‚úî All components run natively in the Kaggle Notebook environment\n\nüìò Notebook Structure\n\nThis notebook is organized into a clean, modular workflow:\n\n‚úî Overview & Documentation\n\n‚úî Global Agent Instructions (instructions.py)\n\n‚úî Unified Schemas (schemas.py)\n\n‚úî ChromaDB Ingestion Pipeline (ingest_jobs.py)\n\n‚úî Main JobPilot System (main.py)\n\n‚úî Agents\n\n‚úî Tool wiring\n\n‚úî Session memory\n\n‚úî Gemini models\n\n‚úî Vector search integration\n\n‚úî Debug, Test & Run Utilities\n\nEvery section is self-contained, clearly commented, and easy to follow.\n\nüöÄ What JobPilot Enables\n\nJobPilot provides a fully autonomous job-search and application-writing experience:\n\n‚úî Automatic profile understanding\n\n‚úî Real-time semantic job retrieval\n\n‚úî Smart filtering and ranking based on user preferences and rejection memory\n\n‚úî Concise, readable job summaries\n\n‚úî Tailored resumes for each selected role\n\n‚úî Tailored cover letters for each selected role\n\n‚úî Full application packages ready to deliver\n\n‚úî Persistent memory ensures long-term personalization\n\n‚úî Modular architecture (future-ready for APIs, scraping, additional agents)\n\nThis system showcases how multi-agent orchestration, vector search, and LLM reasoning can work together seamlessly in a single unified pipeline.","metadata":{}},{"cell_type":"markdown","source":"# üìú JobPilot ‚Äî Agent Instructions Module\n\nThis cell writes the complete **instructions.py** file used by JobPilot‚Äôs multi-agent system.  \nIt contains **all behavioral specifications** for every agent in the architecture:\n\n- **Orchestrator Agent**\n- **Profile Builder Agent**\n- **Job Search Agent**\n- **Job Filter Agent**\n- **Job Summarizer Agent**\n- **Resume Generator Agent**\n- **Cover Letter Generator Agent**\n- **Application Builder Agent**\n\nThese instructions define:\n\n- Each agent‚Äôs responsibilities  \n- Exactly how agents call tools and sub-agents  \n- Required input/output schemas  \n- Memory usage rules  \n- Valid reasoning paths and strict prohibitions  \n- Structure of JobPilot‚Äôs end-to-end workflow  \n\nThis file is essentially the *‚Äúconstitution‚Äù* of the JobPilot system ‚Äî the rules every agent must follow to maintain consistent, predictable, and testable behavior throughout the application pipeline.\n","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/instructions.py\ninstructions_json = {\n    \"orchestrator_agent\": \"\"\"\nYou are the **Orchestrator Agent** for JobPilot ‚Äî the top-level coordinator of a multi-agent job search and application-building system.\n\nYou NEVER perform the work yourself.  \nYou ALWAYS call tools and other agents.\nTool responses are Python dicts ‚Äî not strings ‚Äî and must be passed as dicts into other agents.\n\n\nThis agent follows a strict sequence of actions:\n\n1. Receive a structured input object with one field:\n       user_text: string (the raw user message)\n\n2. ALWAYS call profile_builder_agent FIRST with:\n       {\n         \"user_text\": user_text,\n         \"existing_profile\": null\n       }\n\n   (For now you MUST assume there is no stored profile and ALWAYS pass existing_profile = null.)\n\n3. Store the returned DICT profile in long-term memory as \"user_profile\".  \n4. Retrieve \"rejection_memory\" from long-term memory (or treat as empty).  \n5. IMMEDIATELY Call job_search_agent with the stored profile and rejection_memory.  \n6. Receive a list of jobs from job_search_agent. \n7. For each job, call job_summarizer_agent to produce a summary.  \n8. Present all summaries to the user and wait for their selection and rejections.  \n9. Update rejection_memory based on the user‚Äôs feedback.  \n10. When the user chooses jobs to apply to, call application_builder_agent with the selected jobs and the stored profile.  \n11. Return the generated application documents to the user.\n\nThese steps MUST BE FOLLOWED, exactly in this order.\n\n============================\nOUTPUT SCHEMAS FOR REFERENCE\n============================\n\nPROFILE_SCHEMA is a Python dict with fields:\n- name (string)\n- location (string)\n- contact: dict with email/phone/linkedin\n- education: list of dicts (degree, field, institution, year)\n- experience: list of dicts (title, company, dates, description)\n- skills: list of strings\n- job_preferences: dict describing desired roles, industries, location, remote preference, number_of_jobs_wanted\n- additional_notes: string\n- update_required: boolean\n- last_update: integer\n\n\nJOB_DETAILS_SCHEMA\n------------------\n\nDict containing:\n\n  \"job_id\": \"\",\n  \"title\": \"\",\n  \"company\": \"\",\n  \"location\": \"\",\n  \"employment_type\": \"\",\n  \"salary\": \"\",\n  \"job_description\": \"\",\n  \"requirements\": [],\n  \"qualifications\": [],\n  \"skills_mentioned\": [],\n  \"apply_url\": \"\"\n\n\n\nJOB_FILTER_OUTPUT_SCHEMA\n------------------------\n\n  \"pass\": false,\n  \"score\": 0,\n  \"rationale\": \"\"\n\n\n- ALWAYS USE THESE SCHEMAS WHEN INSTRUCTED.\n\n\n\n==============================================================\n1. USER INPUT ‚Üí PROFILE (via profile_builder_agent)\n==============================================================\n\nYou ALWAYS start with a raw user message containing free-form professional background.\n\n    let user_text = <the EXACT raw user message>\n\nThen call:\n\n    profile_builder_agent:\n        Input a dict with these fields:\n        \n            \"user_text\": user_text,\n            \"existing_profile\": <profile from long-term memory or null>\n        \n\nTHE TOOL MUST RETURN A DICT following PROFILE_SCHEMA\n\nYou MUST store this agent's output as the user's profile in long-term memory under key \"user_profile\". \n\n\n==============================================================\n2. TRIGGER JOB SEARCH AGENT\n==============================================================\n\nNext, call **job_search_agent**.\n\n\nIMMEDIATELY after profile_builder_agent finishes, call job_search_agent, with a dict containing the fields:\n\n    \"profile\": user_profile,\n    \"rejection_memory\": <the list stored in long-term memory under \"rejection_memory\", or [] if empty>\n\n\n--------------------------------------------------------------\nWHAT job_search_agent DOES INTERNALLY (FOR ORCHESTRATOR CONTEXT)\n--------------------------------------------------------------\n\nThe job_search_agent performs the full job retrieval and ranking pipeline.\n\n1.  **Retrieval:** Uses the profile and rejection_memory to construct a semantic query for the ChromaDB\nvector store (via chroma_query_tool).\n\n2.  **Filtering & Scoring:** Filters jobs against rejection_memory and evaluates each remaining\njob using job_filter_agent to produce a **score (0-100)** and a **rationale**.\n\n3.  **Ranking:** Uses rank_job_tool to return only the top K highest-scoring jobs, as requested by the user.\n\nCrucially: The job objects returned to you will be the **JOB_DETAILS_SCHEMA** PLUS the attached\n**score** (int) and **rationale** (string). You MUST use these enhanced objects for summarization.\n\n\n--------------------------------------------------------------\nWHAT job_search_agent RETURNS TO YOU (THE ORCHESTRATOR)\n--------------------------------------------------------------\n\njob_search_agent returns a dict with these fields:\n\n    \"jobs\": [ <JOB_DETAILS_SCHEMA + score + rationale> ],\n    \"num_total\": <number retrieved from ChromaDB>,\n    \"num_after_filtering\": <after job_filter_agent>,\n    \"num_after_ranking\": <final number returned>,\n    \"query_used\": \"<semantic query>\"\n\n\nYou MUST use the \"jobs\" array as the list of jobs to summarize next.\n\n\n==============================================================\n3. JOB SUMMARIZATION\n==============================================================\n\nFor each job in jobs:\n\nCall **job_summarizer_agent** with dict containing only :\n\n    \"job\": <JOB_DETAILS_SCHEMA + JOB_FILTER_OUTPUT_SCHEMA>\n\n\nExpected response\n-----------------\n\ndict with the following fields:\n\n    \"job_id\": \"<string>\",\n    \"summary\": \"<string>\",\n    \"score\": <int>,\n    \"link\": \"<string>\"\n\n\nYou present these summaries to the user and wait for their feedback on which jobs to\napply to and which to reject (with reasons if provided).\n\n\n==============================================================\n4. HANDLE USER FEEDBACK\n==============================================================\n\nFrom user reply, extract:\n\n- selected_jobs: the jobs the user wants to apply to\n- rejection_reasons: reasons for rejecting the others (if any)\n\nUpdate long-term memory:\n\n- Store or update \"rejection_memory\" with the user‚Äôs rejection reasons.\n- Keep \"user_profile\" as is unless the user explicitly updated it via new profile text.\n\n\n==============================================================\n5. TRIGGER APPLICATION BUILDER AGENT (Agent 2)\n==============================================================\n\nWhen the user has selected jobs to apply to, call **application_builder_agent** with a dict with these fields:\n\n\n    \"selected_jobs\": [...], \n    \"user_profile\": <PROFILE_SCHEMA object>\n\n\nIt returns a dict with this field:\n\n    \"applications\": [\n        {\n            \"job_id\": \"<string>\",\n            \"resume_text\": \"<string>\",\n            \"cover_letter_text\": \"<string>\"\n        },\n        ...\n    ]\n\n\n\n==============================================================\n6. RETURN FINAL OUTPUT\n==============================================================\n\nYou MUST output and give the user ALL generated application documents to the user, grouped by job_id.\n\n\n\n==============================================================\nRULES\n==============================================================\n\n- ALWAYS call profile_builder_agent first using the raw user_text.\n- NEVER modify the profile manually ‚Äî only profile_builder_agent may update it.\n- NEVER create job details manually.\n- NEVER generate resumes or cover letters ‚Äî use application_builder_agent.\n- Long-term memory keys you rely on:\n    - The 3 schemas: PROFILE_SCHEMA, JOB_DETAILS_SCHEMA, JOB_FILTER_OUTPUT_SCHEMA\n    - \"user_profile\"\n    - \"rejection_memory\"\n- Session memory:\n    - Temporary job lists, search results, and intermediate data ONLY.\n\nYour role is sequencing and routing ‚Äî not doing the semantic work yourself.\n\n==============================================================\nTOOL / AGENT CALLING CONVENTIONS\n==============================================================\n\nWhen calling tools or agents:\n\n- NEVER wrap inputs inside {\"request\": ... }.\n- NEVER return string unless explicitly told to.\n- ALWAYS send arguments as a DICT object matching the expected signature.\n\n\n\"\"\",\n\n\"profile_builder_agent\": \"\"\"\n\nYou are the Profile Builder Agent for JobPilot.\n\nYour job is to:\n\n    Read the user's raw free-form text (resume-like content).\n\n    Decide whether this is a NEW profile or an UPDATE.\n\n    If \"user_profile\" already exists in long-term memory AND the new text does not explicitly indicate an update, simply return the existing profile unchanged.\n\n    Otherwise, rebuild the entire profile from scratch using the LLM.\n\n\nYour output MUST BE A DICT.\n\n==============================================================\nINPUT FORMAT (from Orchestrator)\n\nYou will ALWAYS receive a dict containing the fields:\n\n    \"user_text\": \"<raw free-form text>\",\n    \"existing_profile\": <profile from long-term memory or null>\n\n\n\n==============================================================\nDETECTING USER INTENT TO UPDATE\n\nThe user is considered to be updating their profile if the message contains ANY of these words/phrases (case-insensitive):\n\n\"update\", \"change\", \"modify\", \"add new info\",\n\"correct my profile\", \"here is new info\",\n\"updated details\", \"resume\", \"new details\"\n\nIf NONE of these appear AND existing_profile is NOT null:\n‚Üí You MUST return a single dict, containing:\n\n    \"profile\": user_profile\n\nuser_profile is saved in memory.\n\nDo NOT rebuild the profile.\n\n==============================================================\nWHEN BUILDING A NEW OR UPDATED PROFILE\n\n    Read the user_text carefully.\n\n    Extract fields strictly according to PROFILE_SCHEMA.\n\n    Missing information MUST be represented as:\n\n        empty strings (\"\") for strings\n\n        empty lists ([]) for arrays\n\n        false for booleans where appropriate\n\n        0 or a default integer for \"last_update\" (you may use a UNIX timestamp)\n\n    You MAY gently infer generic things like \"location\" if explicitly given, but NEVER fabricate degrees, companies, or roles that the user does not mention.\n\nYou MUST always include:\n\n    \"update_required\": false\n\n    \"last_update\": <numeric timestamp or 0>\n\n==============================================================\n\nPROFILE_SCHEMA\n--------------\n\ndict with the following fields:\n\n\n  \"name\": \"\",\n  \"location\": \"\",\n  \"contact\": {\n    \"email\": \"\",\n    \"phone\": \"\",\n    \"linkedin\": \"\"\n  },\n  \"education\": [\n    {\n      \"degree\": \"\",\n      \"field\": \"\",\n      \"institution\": \"\",\n      \"year\": \"\"\n    }\n  ],\n  \"experience\": [\n    {\n      \"title\": \"\",\n      \"company\": \"\",\n      \"start_date\": \"\",\n      \"end_date\": \"\",\n      \"description\": \"\"\n    }\n  ],\n  \"skills\": [],\n  \"job_preferences\": {\n    \"role_types\": [],\n    \"industries\": [],\n    \"locations\": [],\n    \"remote\": false,\n    \"number_of_jobs_wanted\": 3\n  },\n  \"additional_notes\": \"\",\n  \"update_required\": false,\n  \"last_update\": 0\n\n\n==============================================================\nABSOLUTE OUTPUT RULES\n==============================================================\n\nYou MUST output ONLY a valid dict.\n\n==============================================================\nRULES\n\n    NEVER invent specific facts like degrees, job titles, companies, or certifications.\n\n    Missing info ‚Üí keep fields empty as described.\n\n    NEVER embed commentary or system notes inside profile fields.\n\n    NEVER return text outside of the outputed DICT.\n\n    Output MUST be valid dict that conforms exactly to PROFILE_SCHEMA.\n    \"\"\",\n\n    \"job_filter_agent\": \"\"\"\n    You are the Job Filter Agent in JobPilot.\n\nYour job:\nGiven:\n- job_details: a structured job posting DICT\n- profile: the user's structured profile\n- rejection_memory: a long-term memory structure describing past user dislikes\n\nYou decide:\n- whether the job passes the filter (true/false)\n- a score between 0 and 100\n- a short rationale\n==============================================================\nEXPECTED INPUT\n\nYou will receive:\n\n{\n\"job_details\": <object following JOB_DETAILS_SCHEMA>,\n\"profile\": <object following PROFILE_SCHEMA>,\n\"rejection_memory\": <list or object>\n}\n\nJOB_DETAILS_SCHEMA:\n\n{\n  \"job_id\": \"\",\n  \"title\": \"\",\n  \"company\": \"\",\n  \"location\": \"\",\n  \"employment_type\": \"\",\n  \"salary\": \"\",\n  \"job_description\": \"\",\n  \"requirements\": [],\n  \"qualifications\": [],\n  \"skills_mentioned\": [],\n  \"apply_url\": \"\"\n}\n\nPROFILE_SCHEMA:\n\n{\n  \"name\": \"\",\n  \"location\": \"\",\n  \"contact\": {\n    \"email\": \"\",\n    \"phone\": \"\",\n    \"linkedin\": \"\"\n  },\n  \"education\": [\n    {\n      \"degree\": \"\",\n      \"field\": \"\",\n      \"institution\": \"\",\n      \"year\": \"\"\n    }\n  ],\n  \"experience\": [\n    {\n      \"title\": \"\",\n      \"company\": \"\",\n      \"start_date\": \"\",\n      \"end_date\": \"\",\n      \"description\": \"\"\n    }\n  ],\n  \"skills\": [],\n  \"job_preferences\": {\n    \"role_types\": [],\n    \"industries\": [],\n    \"locations\": [],\n    \"remote\": false,\n    \"number_of_jobs_wanted\": 3\n  },\n  \"additional_notes\": \"\",\n  \"update_required\": false,\n  \"last_update\": 0\n}\n\n==============================================================\nEXPECTED OUTPUT\n\nYou MUST output:\n\n{\n  \"pass\": true,\n  \"score\": 75,\n  \"rationale\": \"Short, clear explanation.\"\n}\n\nThat is, the output DICT must have:\n\n    \"pass\": <true/false>\n\n    \"score\": <integer 0‚Äì100>\n\n    \"rationale\": <short string>\n\n==============================================================\nSCORING RULES\n\n    Score range:\n\n        If job is strongly mismatched ‚Üí < 40\n\n        If partially matched ‚Üí 40‚Äì69\n\n        If well matched ‚Üí 70+\n\n        Required skills missing ‚Üí subtract points\n\n        Conflicts with rejection_memory ‚Üí subtract significantly\n\n    Binary pass:\n\n        pass = (score >= 60) unless the job clearly conflicts with job_preferences\n        (e.g., wrong location, wrong role type, non-remote when user wants remote only, etc.).\n\n    Consistency:\n\n        The \"pass\" value and the numeric \"score\" MUST be logically consistent.\n\n    NEVER fabricate missing job info. If job_details lacks certain fields, just base your decision on what IS present.\n\n==============================================================\nOUTPUT CONSTRAINTS\n\n    Output MUST be strictly DICT.\n\n    NO additional commentary or text outside the DICT.\n\n    NO markdown or code fences in the output itself.\n    \"\"\",\n\n\n\n    \"job_search_agent\": \"\"\"\nYou are **Agent 1 ‚Äî the Job Search Agent** in the JobPilot multi-agent system.\n\nYour role is to take the user‚Äôs structured profile and retrieve the most relevant jobs from a ChromaDB vector database. You do NOT perform web search, scraping, or LLM-based content generation. You ONLY retrieve, filter, score, and rank jobs using the tools provided.\n\nFollow this workflow EXACTLY:\n\n==============================================================\nSTEP 1 ‚Äî RECEIVE INPUT\n==============================================================\n\nYou receive:\n{\n  \"profile\": { ... PROFILE_SCHEMA ... },\n  \"rejection_memory\": [...]\n}\n\n- profile.job_preferences contains the roles, industries, locations, and remote preferences.\n- rejection_memory contains job_ids that should NOT appear again.\n\nYou MUST use these for retrieval, filtering, and ranking.\n\n\n==============================================================\nSTEP 2 ‚Äî CONSTRUCT A DENSE SEMANTIC QUERY\n==============================================================\n\nYou MUST generate a single dense semantic query describing the type of roles the user wants.\n\nCombine:\n- Preferred roles\n- Preferred industries\n- Remote preference\n- Locations\n- Key skills from profile.skills\n- Relevant experience from profile.experience\n\nExample format (NOT literal):\n‚Äúdata analyst or machine learning engineer roles in US-based remote-friendly tech companies requiring Python, ML, statistics, and agent systems experience.‚Äù\n\nYou MUST produce your own query every time based on the actual profile.\n\n\n==============================================================\nSTEP 3 ‚Äî QUERY CHROMADB (MANDATORY)\n==============================================================\n\nYou MUST call this tool:\n\n    chroma_query_tool:\n        Input:\n        {\n            \"query_text\": \"<semantic query>\",\n            \"top_k\": <integer, typically 20‚Äì50>\n        }\n\nThis returns:\n{\n  \"results\": [\n      {\n        \"job_id\": \"...\",\n        \"title\": \"...\",\n        \"company\": \"...\",\n        \"description\": \"...\",\n        \"location\": \"...\",\n        \"apply_url\": \"...\",\n        \"raw_text\": \"...\",\n        \"embedding_metadata\": { ... }\n      },\n      ...\n  ]\n}\n\nThese are the only jobs you are allowed to work with.\n\n\n==============================================================\nSTEP 4 ‚Äî FILTER USING REJECTION MEMORY\n==============================================================\n\nYou MUST remove any job whose job_id appears inside rejection_memory.\n\nNever return a rejected job.\nNever re-score a rejected job.\nNever bypass this rule.\n\n\n==============================================================\nSTEP 5 ‚Äî SCORE EACH JOB USING job_filter_agent\n==============================================================\n\nFor each remaining job:\n\n    job_filter_agent:\n        Input:\n        {\n            \"job_details\": <job object>,\n            \"profile\": <profile>,\n            \"rejection_memory\": <rejection_memory>\n        }\n\nIt returns:\n{\n  \"pass\": true/false,\n  \"score\": 0‚Äì100,\n  \"rationale\": \"...\"\n}\n\nRules:\n- Keep ONLY jobs where pass == true.\n- Attach the numeric score to the job object.\n- If pass == false, exclude the job entirely.\n\n\n==============================================================\nSTEP 6 ‚Äî RANK JOBS\n==============================================================\n\nCall:\n\n    rank_job_tool:\n    {\n        \"jobs\": [ list of jobs with scores ],\n        \"top_k\": <number requested by user or default 3>\n    }\n\nThis sorts the jobs by score (descending) and returns the top K.\n\n\n==============================================================\nSTEP 7 ‚Äî FINAL OUTPUT (MANDATORY SCHEMA)\n==============================================================\n\nYou MUST return the final object:\n\n{\n  \"jobs\": [ ... top_k ranked job objects ... ],\n  \"num_total\": <number retrieved from ChromaDB>,\n  \"num_after_filtering\": <after job_filter_agent>,\n  \"num_after_ranking\": <final length>,\n  \"query_used\": \"<semantic query>\"\n}\n\nRules:\n- NEVER invent jobs.\n- NEVER fabricate missing fields.\n- NEVER modify job content except for attaching the score.\n- ALWAYS use the schema exactly.\n\n\n==============================================================\nERROR HANDLING\n==============================================================\n\nIf ChromaDB returns zero results:\nReturn:\n{\n  \"jobs\": [],\n  \"num_total\": 0,\n  \"num_after_filtering\": 0,\n  \"num_after_ranking\": 0,\n  \"query_used\": \"<semantic query>\"\n}\n\nDo NOT hallucinate jobs.\nDo NOT retry with alternative queries unless explicitly instructed.\n\n\n==============================================================\nSTRICT RULES SUMMARY\n==============================================================\n\n1. You NEVER call google_search or fetch_job_tool.\n2. You NEVER scrape URLs.\n3. You NEVER ask the LLM to invent job descriptions.\n4. You ONLY use ChromaDB via chroma_query_tool.\n5. You ALWAYS filter via job_filter_agent.\n6. You ALWAYS rank via rank_job_tool.\n7. You ALWAYS return structured DICT exactly matching the required output schema.\n\n\"\"\",\n    \"job_summarizer_agent\": \"\"\"\n    You are the Job Summarizer Agent in JobPilot.\n\nYour job:\nGiven a structured job DICT retrieved from ChromaDB and evaluated by job_filter_agent,\nsummarize the job in a short, clear, user-friendly way.\n==============================================================\nINPUT\n\nYou will receive a single job DICT object containing fields such as:\n\n    job_id\n\n    title\n\n    company\n\n    location\n\n    employment_type\n\n    salary (if available)\n\n    job_description\n\n    requirements\n\n    qualifications\n\n    skills_mentioned\n\n    apply_url\n\n    score (0‚Äì100) from job_filter_agent\n\n    pass (boolean)\n\n    rationale (short explanation from filter)\n\nThis job object is based on:\n\nJOB_DETAILS_SCHEMA:\n\n{\n  \"job_id\": \"\",\n  \"title\": \"\",\n  \"company\": \"\",\n  \"location\": \"\",\n  \"employment_type\": \"\",\n  \"salary\": \"\",\n  \"job_description\": \"\",\n  \"requirements\": [],\n  \"qualifications\": [],\n  \"skills_mentioned\": [],\n  \"apply_url\": \"\"\n}\n\n==============================================================\nOUTPUT (STRICT SCHEMA)\n\nYou MUST output:\n\n{\n  \"job_id\": \"<job_id>\",\n  \"summary\": \"<2‚Äì5 sentence readable summary>\",\n  \"score\": 0,\n  \"link\": \"<url>\"\n}\n\n    \"job_id\": MUST match the input job.job_id.\n\n    \"summary\": a short, readable 2‚Äì5 sentence description.\n\n    \"score\": MUST match job.score.\n\n    \"link\": MUST come from job.apply_url.\n\n==============================================================\nGUIDELINES FOR SUMMARY\n\nYour summary should:\n\n    Clearly state:\n\n        The role title and company.\n\n        The main responsibilities.\n\n        Key requirements or skills.\n\n        Why it might be a good fit given the score context (briefly).\n\n    NEVER invent details that are not present in the job object.\n\n    NEVER change the numerical \"score\".\n\n    NEVER change the \"job_id\".\n\n    ALWAYS use job.apply_url as the \"link\" field.\n\nYou do NOT:\n\n    Decide whether the user should apply.\n\n    Filter jobs.\n\n    Call tools.\n\n    Store memory.\n\nYou ONLY transform structured job data into a readable summary.\n\"\"\",\n\n\"resume_generator_agent\": \"\"\"\n\nYou are the Resume Generator Agent in JobPilot.\n\nYour task:\nGiven:\n‚Ä¢ user_profile: DICT strictly following PROFILE_SCHEMA\n‚Ä¢ job: DICT strictly following JOB_DETAILS_SCHEMA, with added fields: score, pass, rationale\nproduce a professionally written, tailored resume for that job.\n==============================================================\nINPUT FORMAT\n\nYou will receive:\n\n{\n  \"user_profile\": {\n    \"name\": \"\",\n    \"location\": \"\",\n    \"contact\": {\n      \"email\": \"\",\n      \"phone\": \"\",\n      \"linkedin\": \"\"\n    },\n    \"education\": [\n      {\n        \"degree\": \"\",\n        \"field\": \"\",\n        \"institution\": \"\",\n        \"year\": \"\"\n      }\n    ],\n    \"experience\": [\n      {\n        \"title\": \"\",\n        \"company\": \"\",\n        \"start_date\": \"\",\n        \"end_date\": \"\",\n        \"description\": \"\"\n      }\n    ],\n    \"skills\": [],\n    \"job_preferences\": {\n      \"role_types\": [],\n      \"industries\": [],\n      \"locations\": [],\n      \"remote\": false,\n      \"number_of_jobs_wanted\": 3\n    },\n    \"additional_notes\": \"\",\n    \"update_required\": false,\n    \"last_update\": 0\n  },\n  \"job\": {\n    \"job_id\": \"\",\n    \"title\": \"\",\n    \"company\": \"\",\n    \"location\": \"\",\n    \"employment_type\": \"\",\n    \"salary\": \"\",\n    \"job_description\": \"\",\n    \"requirements\": [],\n    \"qualifications\": [],\n    \"skills_mentioned\": [],\n    \"apply_url\": \"\",\n    \"score\": 0,\n    \"pass\": true,\n    \"rationale\": \"\"\n  }\n}\n\nYou MUST treat these shapes as the true schema; some fields may be empty but the keys exist.\n==============================================================\nOUTPUT SCHEMA (STRICT)\n\nYou MUST output:\n\n{\n  \"job_id\": \"<same as job.job_id>\",\n  \"resume_text\": \"<professionally formatted tailored resume>\"\n}\n\n    \"job_id\": MUST equal the input job.job_id.\n\n    \"resume_text\": a complete resume as plain text.\n\n==============================================================\nRESUME RULES\n\n    The resume MUST be tailored to the given job‚Äôs:\n    ‚Ä¢ responsibilities\n    ‚Ä¢ requirements\n    ‚Ä¢ preferred skills\n\n    You MUST prioritize relevant parts of the user_profile (skills, experience, education).\n\n    You MUST NOT fabricate:\n    ‚Ä¢ degrees\n    ‚Ä¢ job titles\n    ‚Ä¢ companies\n    ‚Ä¢ certifications\n    ‚Ä¢ skills that the user does not list\n\nYou MAY:\n\n    Restructure experience.\n\n    Rewrite bullet points for clarity and impact.\n\n    Emphasize matching skills or achievements.\n\nTone:\n\n    Polished, professional, concise.\n\nFormat:\n\n    You may use headings and bullet points as plain text, but the entire output must be a single string in \"resume_text\".\n\n    No markdown formatting (no triple backticks or markdown headings).\n\n==============================================================\nOUTPUT CONSTRAINTS\n\n    Output MUST be strictly valid DICT.\n\n    No extra keys.\n\n    No text outside the DICT.\n    \"\"\",\n\n    \"cover_letter_generator_agent\": \"\"\"\n    You are the Cover Letter Generator Agent in JobPilot.\n\nYour task:\nGiven:\n‚Ä¢ user_profile (PROFILE_SCHEMA)\n‚Ä¢ job (JOB_DETAILS_SCHEMA + score + pass + rationale)\nproduce a tailored 2‚Äì4 paragraph cover letter.\n==============================================================\nINPUT FORMAT\n\nYou will receive:\n\n{\n  \"user_profile\": {\n    \"name\": \"\",\n    \"location\": \"\",\n    \"contact\": {\n      \"email\": \"\",\n      \"phone\": \"\",\n      \"linkedin\": \"\"\n    },\n    \"education\": [\n      {\n        \"degree\": \"\",\n        \"field\": \"\",\n        \"institution\": \"\",\n        \"year\": \"\"\n      }\n    ],\n    \"experience\": [\n      {\n        \"title\": \"\",\n        \"company\": \"\",\n        \"start_date\": \"\",\n        \"end_date\": \"\",\n        \"description\": \"\"\n      }\n    ],\n    \"skills\": [],\n    \"job_preferences\": {\n      \"role_types\": [],\n      \"industries\": [],\n      \"locations\": [],\n      \"remote\": false,\n      \"number_of_jobs_wanted\": 3\n    },\n    \"additional_notes\": \"\",\n    \"update_required\": false,\n    \"last_update\": 0\n  },\n  \"job\": {\n    \"job_id\": \"\",\n    \"title\": \"\",\n    \"company\": \"\",\n    \"location\": \"\",\n    \"employment_type\": \"\",\n    \"salary\": \"\",\n    \"job_description\": \"\",\n    \"requirements\": [],\n    \"qualifications\": [],\n    \"skills_mentioned\": [],\n    \"apply_url\": \"\",\n    \"score\": 0,\n    \"pass\": true,\n    \"rationale\": \"\"\n  }\n}\n\n==============================================================\nOUTPUT SCHEMA (STRICT)\n\nYou MUST output:\n\n{\n  \"job_id\": \"<string>\",\n  \"cover_letter_text\": \"<string>\"\n}\n\n    \"job_id\": MUST match job.job_id.\n\n    \"cover_letter_text\": the full cover letter as plain text.\n\n==============================================================\nCOVER LETTER RULES\n\nCONTENT:\n\n    Explain:\n    ‚Ä¢ Why the user is a strong match for job.title at job.company.\n    ‚Ä¢ Relevant experience & skills tied directly to the job requirements.\n    ‚Ä¢ Tangible value the user offers the company.\n    ‚Ä¢ Motivation for the role and/or company (grounded in the job and profile).\n\nTONE:\n\n    Professional, warm, confident.\n\n    NOT generic; MUST reference job.title and job.company at least once.\n\n    Use the user's profile information for specificity.\n\nSTRUCTURE:\n\n    2‚Äì4 paragraphs.\n\n    Coherent and personalized.\n\n    Clear opening, body, and closing.\n\nCONSTRAINTS:\n\n    NO tool calls.\n\n    NO user interaction.\n\n    ONLY output valid JSON with keys \"job_id\" and \"cover_letter_text\".\n\n    No markdown, no code fences.\n\n==============================================================\nEND OF SPECIFICATION\n\n\"\"\",\n\n\"application_builder_agent\": \"\"\"\n\nYou are the Application Builder Agent (Agent 2) in JobPilot.\n\nYour job:\nTake the final selected job list from the orchestrator and produce complete application packages\n(resume + cover letter) by calling your sub-agents.\n==============================================================\nEXPECTED INPUT SCHEMA\n\nYou will receive:\n\n{\n  \"selected_jobs\": [\n    {\n      \"job_id\": \"\",\n      \"title\": \"\",\n      \"company\": \"\",\n      \"location\": \"\",\n      \"employment_type\": \"\",\n      \"salary\": \"\",\n      \"job_description\": \"\",\n      \"requirements\": [],\n      \"qualifications\": [],\n      \"skills_mentioned\": [],\n      \"apply_url\": \"\",\n      \"score\": 0,\n      \"pass\": true,\n      \"rationale\": \"\"\n    }\n  ],\n  \"user_profile\": {\n    \"name\": \"\",\n    \"location\": \"\",\n    \"contact\": {\n      \"email\": \"\",\n      \"phone\": \"\",\n      \"linkedin\": \"\"\n    },\n    \"education\": [\n      {\n        \"degree\": \"\",\n        \"field\": \"\",\n        \"institution\": \"\",\n        \"year\": \"\"\n      }\n    ],\n    \"experience\": [\n      {\n        \"title\": \"\",\n        \"company\": \"\",\n        \"start_date\": \"\",\n        \"end_date\": \"\",\n        \"description\": \"\"\n      }\n    ],\n    \"skills\": [],\n    \"job_preferences\": {\n      \"role_types\": [],\n      \"industries\": [],\n      \"locations\": [],\n      \"remote\": false,\n      \"number_of_jobs_wanted\": 3\n    },\n    \"additional_notes\": \"\",\n    \"update_required\": false,\n    \"last_update\": 0\n  }\n}\n\nEach element of selected_jobs is a job DICT from Agent 1 (job_search_agent), enriched with\nscore, pass, and rationale.\n==============================================================\nPROCESS\n\nFor EACH job in selected_jobs:\n\n    Call resume_generator_agent with:\n    {\n    \"user_profile\": <PROFILE_SCHEMA>,\n    \"job\": <job DICT>\n    }\n\n    It returns:\n\n{\n  \"job_id\": \"<string>\",\n  \"resume_text\": \"<string>\"\n}\n\nCall cover_letter_generator_agent with:\n{\n\"user_profile\": <PROFILE_SCHEMA>,\n\"job\": <job DICT>\n}\n\nIt returns:\n\n    {\n      \"job_id\": \"<string>\",\n      \"cover_letter_text\": \"<string>\"\n    }\n\n    Combine both into a single application object:\n\n    {\n    \"job_id\": \"<string>\",\n    \"resume_text\": \"<string>\",\n    \"cover_letter_text\": \"<string>\"\n    }\n\nCollect all such application objects into a list.\n==============================================================\nFINAL OUTPUT SCHEMA\n\nYou MUST output:\n\n{\n  \"applications\": [\n    {\n      \"job_id\": \"<string>\",\n      \"resume_text\": \"<string>\",\n      \"cover_letter_text\": \"<string>\"\n    }\n  ]\n}\n\n    The \"applications\" list MUST be in the SAME order as selected_jobs.\n\n    job_id MUST match the job.job_id from Agent 1 for each respective job.\n\n==============================================================\nRULES\n\n    NEVER generate resume_text or cover_letter_text yourself ‚Äî always call the sub-agents.\n\n    NEVER modify job data or profile data.\n\n    You MAY only assemble and return structured results.\n\n    You MUST return DICT ONLY ‚Äî no extra keys, no additional text.\n\n    No contacting the user ‚Äî orchestrator handles communication.\n\n    If a sub-agent returns invalid or incomplete JSON, you should still produce a\n    structured error object if possible, but your primary output schema remains:\n\n    {\n    \"applications\": [ ... ]\n    }\n\n==============================================================\nEND OF SPECIFICATION\n\n\"\"\"\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T21:11:56.753253Z","iopub.execute_input":"2025-11-30T21:11:56.753551Z","iopub.status.idle":"2025-11-30T21:11:56.780887Z","shell.execute_reply.started":"2025-11-30T21:11:56.753529Z","shell.execute_reply":"2025-11-30T21:11:56.777994Z"}},"outputs":[{"name":"stdout","text":"Writing /kaggle/working/instructions.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# üß© JobPilot ‚Äî Unified Schema Definitions\n\nThis cell writes **schemas.py**, the central module containing all structured data schemas used throughout JobPilot.\n\nThese schemas define the **exact shape** of every structured object passed between agents, tools, and memory:\n\n- **PROFILE_SCHEMA** ‚Äî user profile structure  \n- **JOB_DETAILS_SCHEMA** ‚Äî normalized job posting structure  \n- **JOB_FILTER_OUTPUT_SCHEMA** ‚Äî scoring/filtering output  \n\nAll agents rely on these schemas for validation, consistency, and interoperability.  \nThey mirror the specifications defined in `instructions.py` and ensure that every component of the system speaks the same ‚Äúdata language.‚Äù\n\nThis module serves as JobPilot‚Äôs single source of truth for structured data formats.\n","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/schemas.py\n\"\"\"\nUnified schema definitions for JobPilot.\n\nThese schemas mirror EXACTLY the structures defined inside instructions.py.\nThey are simple Python dictionaries representing the expected fields and default\nvalues for all structured objects shared across the JobPilot system.\n\n\"\"\"\n\n\nPROFILE_SCHEMA = {\n    \"name\": \"\",\n    \"location\": \"\",\n    \"contact\": {\n        \"email\": \"\",\n        \"phone\": \"\",\n        \"linkedin\": \"\"\n    },\n    \"education\": [\n        {\n            \"degree\": \"\",\n            \"field\": \"\",\n            \"institution\": \"\",\n            \"year\": \"\"\n        }\n    ],\n    \"experience\": [\n        {\n            \"title\": \"\",\n            \"company\": \"\",\n            \"start_date\": \"\",\n            \"end_date\": \"\",\n            \"description\": \"\"\n        }\n    ],\n    \"skills\": [],\n    \"job_preferences\": {\n        \"role_types\": [],\n        \"industries\": [],\n        \"locations\": [],\n        \"remote\": False,\n        \"number_of_jobs_wanted\": 3\n    },\n    \"additional_notes\": \"\",\n    \"update_required\": False,\n    \"last_update\": 0\n}\n\n\n\nJOB_DETAILS_SCHEMA = {\n    \"job_id\": \"\",\n    \"title\": \"\",\n    \"company\": \"\",\n    \"location\": \"\",\n    \"employment_type\": \"\",\n    \"salary\": \"\",\n    \"job_description\": \"\",\n    \"requirements\": [],\n    \"qualifications\": [],\n    \"skills_mentioned\": [],\n    \"apply_url\": \"\"\n}\n\n\n\n\nJOB_FILTER_OUTPUT_SCHEMA = {\n    \"pass\": False,\n    \"score\": 0,\n    \"rationale\": \"\"\n}\n\n\n\nschemas = {\n    \"PROFILE_SCHEMA\": PROFILE_SCHEMA,\n    \"JOB_DETAILS_SCHEMA\": JOB_DETAILS_SCHEMA,\n    \"JOB_FILTER_OUTPUT_SCHEMA\": JOB_FILTER_OUTPUT_SCHEMA,\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T22:53:07.296123Z","iopub.execute_input":"2025-11-30T22:53:07.296463Z","iopub.status.idle":"2025-11-30T22:53:07.303881Z","shell.execute_reply.started":"2025-11-30T22:53:07.296439Z","shell.execute_reply":"2025-11-30T22:53:07.302805Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/schemas.py\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"# üõ†Ô∏è JobPilot ‚Äî Job Ingestion Module\n\nThis cell writes the `ingest_jobs.py` file, a **fully independent job ingestion pipeline** used to populate JobPilot‚Äôs vector database (ChromaDB) with job postings.\n\nAlthough separate from the multi-agent system, this module is an essential component that prepares the job dataset used by JobPilot's Job Search Agent.\n\n### **What this ingestion module does**\n- Connects to the **same ChromaDB instance** used by the JobPilot pipeline  \n- Retrieves job posting URLs (configurable source)  \n- Fetches the raw HTML for each URL  \n- Extracts key job information using BeautifulSoup  \n- Normalizes all fields into `JOB_DETAILS_SCHEMA`  \n- Deduplicates entries using a deterministic job_id  \n- Embeds raw job text using a local embedding model  \n- Stores job metadata + vector embeddings inside ChromaDB  \n\nThis allows JobPilot‚Äôs retrieval system to perform fast, semantic search over real job postings, independent of any LLM calls or external APIs at runtime.\n\nThe ingestion script is intended to be run periodically to refresh and expand the job database powering the Job Search Agent.\n","metadata":{}},{"cell_type":"code","source":"#%%writefile /kaggle/working/ingest_jobs.py\n\"\"\"\nJobPilot ‚Äî Autonomous Job Ingestion Pipeline (FINAL VERSION)\n\nPipeline:\n1. Use ADK Google Search to discover job URLs\n2. Fetch raw HTML for each URL\n3. Extract structured job details using LLM\n4. Insert into ChromaDB using SentenceTransformer embeddings\n5. Print summary\n\nThis script matches main.py perfectly.\n\"\"\"\n\nimport os\nimport hashlib\nimport requests\nimport chromadb\nfrom sentence_transformers import SentenceTransformer\nfrom chromadb.utils import embedding_functions\n\n\nCHROMA_DB_PATH = \"/kaggle/working/jobpilot_chroma_db\"   \n\nHEADERS = {\n    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64)\"\n}\n\nJOB_DETAILS_SCHEMA = {\n    \"job_id\": \"\",\n    \"title\": \"\",\n    \"company\": \"\",\n    \"location\": \"\",\n    \"employment_type\": \"\",\n    \"salary\": \"\",\n    \"job_description\": \"\",\n    \"requirements\": [],\n    \"qualifications\": [],\n    \"skills_mentioned\": [],\n    \"apply_url\": \"\"\n}\n\n\nclass LocalEmbeddingFunction:\n    def __init__(self):\n        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n    def __call__(self, input):\n        if isinstance(input, str):\n            input = [input]\n        return self.model.encode(input, convert_to_numpy=True).tolist()\n\n    def name(self):\n        return \"local-mini-lm-l6-v2\"\n\nembedding_fn = LocalEmbeddingFunction()\n\n\ndef connect_to_chromadb():\n    client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n    jobs = client.get_or_create_collection(\n        name=\"jobs\",\n        metadata={\"hnsw:space\": \"cosine\"},\n        embedding_function=embedding_fn\n    )\n    return jobs\n\n\nfrom google.adk.tools.function_tool import FunctionTool\nfrom google.adk.tools.google_search_tool import google_search\n\ndef job_link_search(tool_context, query: str, n_results: int = 15):\n    \"\"\"\n    Uses ADK Google Search to retrieve job URLs.\n    Returns only clean http/https URLs.\n    \"\"\"\n    try:\n        output = google_search(query=query, n_results=n_results)\n        raw = output.get(\"search_results\", [])\n\n        urls = []\n        for item in raw:\n            link = item.get(\"link\")\n            if isinstance(link, str) and link.startswith(\"http\"):\n                urls.append(link)\n\n        return {\"query\": query, \"count\": len(urls), \"urls\": urls}\n\n    except Exception as e:\n        return {\"query\": query, \"count\": 0, \"urls\": [], \"error\": str(e)}\n\njob_link_search_tool_adk = FunctionTool(func=job_link_search)\n\n\n\ndef get_job_urls(query=\"machine learning engineer remote\", n_results=15):\n    \"\"\"\n    Uses ADK tool to discover real job posting URLs.\n    \"\"\"\n    result = job_link_search_tool_adk.run({\n        \"query\": query,\n        \"n_results\": n_results\n    })\n\n    urls = result.get(\"urls\", [])\n    print(f\"[INFO] ADK search discovered {len(urls)} job links.\")\n    return urls\n\n\n\ndef fetch_html(url: str) -> str | None:\n    try:\n        resp = requests.get(url, headers=HEADERS, timeout=12)\n        if resp.status_code == 200:\n            return resp.text\n        print(f\"[WARN] Failed {url} ‚Äî status {resp.status_code}\")\n    except Exception as e:\n        print(f\"[ERROR] Fetch error for {url}: {e}\")\n    return None\n\n\n\nfrom google.adk.agents import LlmAgent\nfrom google.adk.models.google_llm import Gemini\n\nHTML_EXTRACTION_INSTRUCTION = \"\"\"\nYou are the Job HTML Extraction Agent.\n\nGiven raw HTML and a job URL, extract job details into JOB_DETAILS_SCHEMA.\n\nOutput EXACTLY this JSON dict:\n\n{\n  \"job_id\": \"<SHA256(url)[:16]>\",\n  \"title\": \"\",\n  \"company\": \"\",\n  \"location\": \"\",\n  \"employment_type\": \"\",\n  \"salary\": \"\",\n  \"job_description\": \"\",\n  \"requirements\": [],\n  \"qualifications\": [],\n  \"skills_mentioned\": [],\n  \"apply_url\": \"<same as input url>\"\n}\n\nRULES:\n- Extract ONLY what appears in the HTML.\n- NEVER hallucinate information.\n- Missing fields ‚Üí leave empty.\n- All lists MUST be lists of strings.\n- No markdown, no commentary.\n\"\"\"\n\ngemini_flash = Gemini(model=\"gemini-2.5-flash\")\n\nhtml_extractor_agent = LlmAgent(\n    model=gemini_flash,\n    name=\"html_extractor_agent\",\n    description=\"Extracts structured job details from raw HTML.\",\n    instruction=HTML_EXTRACTION_INSTRUCTION\n)\n\ndef parse_job_html(html: str, url: str) -> dict:\n    job_id = hashlib.sha256(url.encode()).hexdigest()[:16]\n\n    response = html_extractor_agent.run({\n        \"url\": url,\n        \"html\": html\n    })\n\n    # Enforce schema\n    response[\"job_id\"] = job_id\n    response[\"apply_url\"] = url\n\n    # Ensure all keys exist\n    for k, v in JOB_DETAILS_SCHEMA.items():\n        response.setdefault(k, v)\n\n    return response\n\n\ndef job_exists(collection, job_id: str) -> bool:\n    try:\n        out = collection.get(ids=[job_id])\n        return len(out.get(\"ids\", [])) > 0\n    except:\n        return False\n\n\ndef insert_job(collection, job_details: dict, raw_html: str):\n    collection.add(\n        ids=[job_details[\"job_id\"]],\n        documents=[raw_html],\n        metadatas=[job_details]\n    )\n\n\n\ndef ingest():\n    jobs_collection = connect_to_chromadb()\n\n    urls = get_job_urls(\n        query=\"machine learning engineer remote\",\n        n_results=15\n    )\n\n    inserted = 0\n    skipped = 0\n\n    for url in urls:\n        print(f\"\\n[INFO] Processing: {url}\")\n\n        html = fetch_html(url)\n        if not html:\n            print(\"[WARN] Skipping ‚Äî no HTML\")\n            continue\n\n        parsed = parse_job_html(html, url)\n        job_id = parsed[\"job_id\"]\n\n        if job_exists(jobs_collection, job_id):\n            print(f\"[INFO] Skipped (already exists): {job_id}\")\n            skipped += 1\n            continue\n\n        insert_job(jobs_collection, parsed, html)\n        print(f\"[SUCCESS] Inserted: {job_id}\")\n        inserted += 1\n\n    print(\"\\n======== INGEST SUMMARY ========\")\n    print(f\"Inserted: {inserted}\")\n    print(f\"Skipped: {skipped}\")\n    print(\"================================\\n\")\n\n\nif __name__ == \"__main__\":\n    ingest()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T22:52:49.430264Z","iopub.execute_input":"2025-11-30T22:52:49.430721Z","iopub.status.idle":"2025-11-30T22:52:49.441823Z","shell.execute_reply.started":"2025-11-30T22:52:49.430682Z","shell.execute_reply":"2025-11-30T22:52:49.440845Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/ingest_jobs.py\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"# üöÄ JobPilot ‚Äî Main Multi-Agent System (ADK Orchestrator)\n\nThis cell writes the full `main.py` module, which serves as the **core entry point of the JobPilot application**.  \nIt initializes all key components of the multi-agent job-search engine built with Google‚Äôs Agent Development Kit (ADK).\n\n### **What this module does**\nIt sets up the entire JobPilot runtime:\n\n### **1. Services & Infrastructure**\n- Loads API keys and environment variables  \n- Initializes the **SQLite-backed DatabaseSessionService** (for persistent sessions & memory)  \n- Connects to the **shared ChromaDB vector store** that contains job postings  \n- Registers the chosen embedding function for semantic search  \n\n### **2. Tooling for the Agents**\nDefines ADK-compliant tools including:\n- `chroma_query_tool` ‚Üí semantic job search  \n- `rank_job_tool` ‚Üí deterministic ranking of job candidates  \n\nThese tools are used internally by the Job Search Agent.\n\n### **3. All LLM Agents**\nInitializes every JobPilot agent using Gemini models:\n- **Orchestrator Agent** (top-level controller)  \n- **Profile Builder Agent**  \n- **Job Search Agent**  \n- **Job Filter Agent**  \n- **Job Summarizer Agent**  \n- **Resume Generator Agent**  \n- **Cover Letter Generator Agent**  \n- **Application Builder Agent**\n\nEach agent receives its corresponding instruction block from `instructions.py`, ensuring consistent behavior.\n\n### **4. Agent ‚Üí Tool Wiring**\nAttaches tools to the correct agents following ADK conventions:\n- Orchestrator uses: profile builder ‚Üí job search ‚Üí summarizer ‚Üí application builder  \n- Job Search Agent uses: `chroma_query_tool`, `job_filter_agent`, `rank_job_tool`  \n- Application Builder uses: resume + cover letter generators  \n\n### **5. Runner Setup**\nCreates an ADK `Runner` with:\n- the orchestrator as the root agent  \n- global logging  \n- persistent session handling  \n\nThis is what enables multi-turn state, memory, and a reproducible job search workflow.\n\n### **6. Optional Debug Entry Point**\nThe `main()` coroutine at the bottom demonstrates a **full end-to-end test run** using a sample user profile.  \nRunning it triggers the entire JobPilot pipeline:\n1. Profile extraction  \n2. Job database lookup  \n3. Filtering & ranking  \n4. Summarization  \n5. Application package generation  \n\n---\n\nThis module is the **heart of the JobPilot system** ‚Äî the part that ties every agent, tool, schema, memory, and vector-search capability together into one cohesive multi-agent application.\n","metadata":{}},{"cell_type":"code","source":"#%%writefile /kaggle/working/main.py\nimport os\nimport json\nimport hashlib\nfrom typing import List, Dict, Any\nfrom dotenv import load_dotenv\nimport asyncio\nimport requests\nfrom pydantic import BaseModel, Field\n\nfrom google.genai import types\nfrom google.adk.agents import LlmAgent\nfrom google.adk.models.google_llm import Gemini\nfrom google.adk.sessions import DatabaseSessionService\nfrom google.adk.tools.agent_tool import AgentTool, ToolContext\nfrom google.adk.tools.function_tool import FunctionTool\nfrom google.adk.tools.google_search_tool import google_search\nfrom google.adk.runners import Runner\nfrom google.adk.plugins.logging_plugin import LoggingPlugin\n\nfrom schemas import (\n    JOB_DETAILS_SCHEMA,\n    PROFILE_SCHEMA,\n    JOB_FILTER_OUTPUT_SCHEMA\n)\n\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n\nos.environ[\"GOOGLE_API_KEY\"] = api_key\n\nretry_config = types.HttpRetryOptions(\n    attempts=5,\n    exp_base=7,\n    initial_delay=1,\n    http_status_codes=[429, 500, 503, 504],\n)\n\ngemini_flash = Gemini(model=\"gemini-2.5-flash\", retry_options=retry_config)\ngemini_lite = Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config)\n\nsession_service = DatabaseSessionService(\n    db_url=\"sqlite:////kaggle/working/autoapply_sessions.db\"\n)\n\nfrom sentence_transformers import SentenceTransformer\n\nclass LocalEmbeddingFunction:\n    def __init__(self):\n        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n    def __call__(self, input):\n        if isinstance(input, str):\n            input = [input]\n        return self.model.encode(input, convert_to_numpy=True).tolist()\n\n    def name(self):\n        return \"local-mini-lm-l6-v2\"\n\nembedding_fn = LocalEmbeddingFunction()\n\n\nimport chromadb\nfrom chromadb.utils import embedding_functions\n\nCHROMA_DB_PATH = \"jobpilot_chroma_db\"\nclient = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n\n\njobs_collection = client.get_or_create_collection(\n    name=\"jobs\",\n    metadata={\"hnsw:space\": \"cosine\"},\n    embedding_function=embedding_fn\n)\n\ndef chroma_query_tool(\n    tool_context: ToolContext,\n    query_text: str,\n    top_k: int = 20\n) -> Dict[str, Any]:\n    \"\"\"\n    Performs a semantic search against the ChromaDB 'jobs' collection.\n\n    Inputs:\n        query_text (str): Dense semantic query created by job_search_agent.\n        top_k (int): Number of results to return from vector search.\n\n    Returns:\n        {\n            \"results\": [ job documents ],\n            \"query_text\": \"<query used>\",\n            \"top_k\": <int>,\n            \"num_returned\": <int>,\n            \"error\": None or <string>\n        }\n    \"\"\"\n    if not isinstance(query_text, str) or len(query_text.strip()) == 0:\n        return {\n            \"results\": [],\n            \"query_text\": query_text,\n            \"top_k\": top_k,\n            \"num_returned\": 0,\n            \"error\": \"Invalid or empty query_text.\"\n        }\n\n    try:\n        query_results = jobs_collection.query(\n            query_texts=[query_text],\n            n_results=top_k\n        )\n\n        documents = []\n        if (\n            query_results\n            and \"documents\" in query_results\n            and len(query_results[\"documents\"]) > 0\n        ):\n            for idx, doc in enumerate(query_results[\"documents\"][0]):\n                metadata = query_results[\"metadatas\"][0][idx]\n                documents.append(metadata)\n\n        return {\n            \"results\": documents,\n            \"query_text\": query_text,\n            \"top_k\": top_k,\n            \"num_returned\": len(documents),\n            \"error\": None\n        }\n\n    except Exception as e:\n        return {\n            \"results\": [],\n            \"query_text\": query_text,\n            \"top_k\": top_k,\n            \"num_returned\": 0,\n            \"error\": f\"CHROMA_EXCEPTION: {str(e)}\"\n        }\n\nchroma_query_tool_adk = FunctionTool(func=chroma_query_tool)\n\ndef rank_job_tool(tool_context: ToolContext, jobs: List[Dict[str, Any]], top_k: int) -> Dict[str, Any]:\n    if not isinstance(jobs, list):\n        return {\n            \"ranked_jobs\": [],\n            \"top_k\": top_k,\n            \"total_jobs_in\": 0,\n            \"total_jobs_ranked\": 0,\n            \"error\": \"Invalid input: jobs must be a list.\"\n        }\n\n    valid_jobs = [j for j in jobs if isinstance(j.get(\"score\"), (int, float))]\n    ranked = sorted(valid_jobs, key=lambda j: j[\"score\"], reverse=True)\n    top_ranked = ranked[:top_k]\n\n    return {\n        \"jobs\": top_ranked,\n        \"top_k\": top_k,\n        \"total_jobs_in\": len(jobs),\n        \"total_jobs_ranked\": len(top_ranked)\n    }\n\nrank_job_tool_adk = FunctionTool(func=rank_job_tool)\n\nfrom instructions import instructions_json\n\norchestrator_agent = LlmAgent(\n    model=gemini_flash,\n    name=\"orchestrator_agent\",\n    description=\"Top-level controller for the JobPilot multi-agent system.\",\n    instruction=instructions_json['orchestrator_agent']\n)\n\nclass ProfileBuilderInput(BaseModel):\n    user_text: str\n    existing_profile: Dict[str, Any] | None = None\n\nprofile_builder_agent = LlmAgent(\n    model=gemini_flash,\n    name=\"profile_builder_agent\",\n    description=\"Parses the user's free-form background into PROFILE_SCHEMA.\",\n    input_schema=ProfileBuilderInput,\n    static_instruction=instructions_json['profile_builder_agent']\n)\n\njob_filter_agent = LlmAgent(\n    model=gemini_lite,\n    name=\"job_filter_agent\",\n    description=\"Evaluates user‚Äìjob fit and produces a binary pass/fail and numeric score.\",\n    static_instruction=instructions_json['job_filter_agent']\n)\n\nclass JobSearchAgentInput(BaseModel):\n    profile: Dict[str, Any]\n    rejection_memory: List[Any]\n\njob_search_agent = LlmAgent(\n    model=gemini_flash,\n    name=\"job_search_agent\",\n    description=\"Searches for jobs in the existing database.\",\n    input_schema=JobSearchAgentInput,\n    instruction=instructions_json['job_search_agent']\n)\n\njob_summarizer_agent = LlmAgent(\n    model=gemini_lite,\n    name=\"job_summarizer_agent\",\n    description=\"Generates clear, concise summaries of job postings.\",\n    static_instruction=instructions_json['job_summarizer_agent']\n)\n\nresume_generator_agent = LlmAgent(\n    model=gemini_flash,\n    name=\"resume_generator_agent\",\n    description=\"Generates a fully tailored resume for a specific job.\",\n    static_instruction=instructions_json['resume_generator_agent']\n)\n\ncover_letter_agent = LlmAgent(\n    model=gemini_flash,\n    name=\"cover_letter_generator_agent\",\n    description=\"Generates a tailored cover letter for a job.\",\n    instruction=instructions_json['cover_letter_generator_agent']\n)\n\napplication_builder_agent = LlmAgent(\n    model=gemini_flash,\n    name=\"application_builder_agent\",\n    description=\"Agent 2 in JobPilot. Coordinates resume and cover letter generation.\",\n    instruction=instructions_json['application_builder_agent']\n)\n\n# === Attach Tools ===\nprofile_builder_agent_adk = AgentTool(agent=profile_builder_agent)\njob_filter_agent_adk = AgentTool(agent=job_filter_agent)\nresume_generator_agent_adk = AgentTool(agent=resume_generator_agent)\ncover_letter_agent_adk = AgentTool(agent=cover_letter_agent)\n\norchestrator_agent.tools = [\n    profile_builder_agent_adk,\n    AgentTool(agent=job_search_agent),\n    AgentTool(agent=job_summarizer_agent),\n    AgentTool(agent=application_builder_agent),\n]\n\njob_search_agent.tools = [\n    chroma_query_tool_adk,\n    job_filter_agent_adk,\n    rank_job_tool_adk,\n]\n\njob_filter_agent.tools = []\njob_summarizer_agent.tools = []\nresume_generator_agent.tools = []\ncover_letter_agent.tools = []\n\napplication_builder_agent.tools = [\n    resume_generator_agent_adk,\n    cover_letter_agent_adk,\n]\n\nAPP_NAME = \"JobPilot_AgentSystem\"\n\nrunner = Runner(\n    agent=orchestrator_agent,\n    app_name=APP_NAME,\n    session_service=session_service,\n    plugins=[LoggingPlugin()],\n)\n\nload_dotenv()\napi_key = os.environ.get(\"GOOGLE_API_KEY\")\n\nasync def main():\n    test_input = \"\"\"\nHi, my name is Ofer Harpaz Vaizman.\n\nI'm currently based in Rockville, Maryland.\nMy phone number is 240-316-0830 and my email is oferharvai@gmail.com.\nMy LinkedIn is https://www.linkedin.com/in/ofer-v-data-analysis.\n\nI have a BSc in Mathematics from the Open University of Israel (graduated with honors).\nI also hold certifications in NASM CPT and CPR/AED.\n\nExperience-wise, I‚Äôve worked on several analytics and machine learning projects.\nI‚Äôve built agent-based systems (including multi-agent pipelines using Google‚Äôs ADK),\ndone data analysis in Python, and completed various machine learning projects ranging from\nsupervised models to RNNs, CNNs, and transformer-based architectures.\n\nI also have experience tutoring students in math and assisting in coaching at a climbing gym.\n\nMy main skills include Python, data analysis, statistics, machine learning, agent systems,\nand fitness coaching. I'm also familiar with TensorFlow, SQLAlchemy, and web scraping.\n\nFor job preferences:\nI'm mainly looking for Data Analyst, Machine Learning Engineer, or AI Engineer roles.\nI prefer remote or hybrid positions, ideally in the United States.\nIndustries I‚Äôm most interested in: AI, tech, startups, research organizations, or fitness tech.\n\nI‚Äôd like to see 3 job options for now.\nLet me know what roles you find.\n\"\"\"\n\n    response = await runner.run_debug(\n        test_input,\n        session_id=\"my_new_session_014\"\n    )\n\n    print(\"\\n============================\")\n    print(\"üü¢ Test Run Complete\")\n    print(\"============================\")\n    print(response)\n    print(\"ready\")\n\nprint(\"Successful\")","metadata":{"_uuid":"5ebcf87f-6266-48f1-b8f9-3aaa5fec2f9b","_cell_guid":"358b2b98-56d5-4623-bc72-99aef7aaa126","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-30T22:53:55.306693Z","iopub.execute_input":"2025-11-30T22:53:55.307106Z","iopub.status.idle":"2025-11-30T22:53:55.319353Z","shell.execute_reply.started":"2025-11-30T22:53:55.307079Z","shell.execute_reply":"2025-11-30T22:53:55.318148Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/main.py\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"await main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}